# IoT Packet Verifier - Design

See [README](../README.md) for overview.

Data flow with AWS S3 elided:

```
 CS ↰
 ↓   \
HPR → PV → Injector
```

where:

- HPR is [Helium Packet Router](https://github.com/helium/helium-packet-router/)
- PV is [this](..) component, the IoT Packet Verifier
- Injector executes debits & credits on blockchain
- CS is the Config Server associated with HPR

## Packet Verifier

- Extract metadata from Protobufs via S3 (read-only)
- Sum packet counters by gateway for Rewards
- Sum packet counters by OUI for debiting DC
- Write output as Protobufs via S3 (create & append only)
- Fetch from Config Server:
  + List of customer OUIs and their NetIDs
  + Wallet addresses per gateway
  + Wallet addresses per OUI
- Notify Config Server:
  + Disable routes when an OUI wallet has insufficient balance
  + Enable routes when an OUI wallet has sufficient balance

## Injector

Injector MVP may be implemented in Erlang and later in Rust

- Operates within time span of a Reward Period:
  + one txn on chain likely greater than one S3 input file
  + approximately 15-30 minutes
- Reads reports generated by PV:
  + wallet addresses are specified within input
  + no further lookups needed
- Creates state-channels on blockchain:
  + credit Rewards to wallets associated with gateways
  + debit DC from wallets associated with OUIs
- Fetch from Config Server:
  + DC rates for debiting customer wallets
  + Rewards rates for crediting gateway wallets

## PV Input Processing

- Fetch NetID to OUI mappings from Config Server
- Lookup OUI wallet balance of those previously suspended
- Sequence of S3 input is in approximate timeseries order
- Fetch wallet info for newly encountered gateway or OUI
- Notify Config Server to suspend an OUI when wallet balance too low
- Update tables by gateway, by OUI:
  + HashMap by gateway -> inc counter for Rewards
  + HashMap by OUI -> inc counter for debiting DC

## PV Output stream

- When each S3 input file ends, write output and reset counters
- Write small atomic blocks containing a header and body
  + because `file_sink.rs` rolls output file after 50MB or 15 minutes

## Dispatcher

- Dispatcher may be Tokio or separate processes-- TBD
  + Trade-off between async+locks vs let OS scheduler do the work
- Watch S3 for new files
- As implicit txn on local OS:
  + Download input file from S3 to local file system
  + Process input file
  + Mark input file on local file system for deletion
  + Push output file to S3
  + Delete marked file
  + Therefore, same node can pick-up where it left off; and
    workflow survives hard stop/start with minimal duplication;
    e.g., when using ephemeral storage
- One worker per S3 input file:
  + Accommodates high concurrency
  + Unlikely to fall behind
  + Cheap GC via `fork()`, `exec()` and `exit()`
    (however, S3 files will be capped at 50MiB before gzip thus small-ish)
